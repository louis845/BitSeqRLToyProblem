{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent environment with bitflipping actions, and random target\n",
    "\n",
    "## Objective\n",
    "The state space is $S=\\{0,1\\}^n$ for fixed $n$. The objective is to find a policy $\\pi_t(s)$ (where the policy is dependent on $t \\in S$) such that we are able to reach $t$ by flipping bits. The actions are represented by $A=\\{0,\\cdots,n-1\\}$ representing which bit to be flipped. The corresponding DQN would be $Q: (s,t,a) \\in S \\times S \\times A \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "## Neural network model details\n",
    " * Input: $\\mathbb{R}^n \\times \\mathbb{R}^n$ vector representing the bit sequence (e.g. $00011 \\sim (0, 0, 0, 1, 1)$) of state and goal.\n",
    " * Output: $\\mathbb{R}^{n}$ vector of the $Q$-values\n",
    "    * The index corresponds to integer $\\geq 0$ representing the index of the bit being flipped\n",
    "    * So $\\text{model}(s,t)[a]$ will be the $Q$-value\n",
    " * Architecture: Simple MLP\n",
    "\n",
    "## Training method\n",
    "Simple DQN with replay\n",
    "   * Exploration step -> update Q network -> validation step\n",
    "\n",
    "**Exploration step**\n",
    "We initialize 16 agents starting at random starting states (for each agent). The actions will be according to the DQN agent, and with probability $\\epsilon$, a random action will be picked uniformly. We add this to the experience buffer. The episode has at most $n$ steps.\n",
    "\n",
    "**Update Q network**\n",
    "Update the DQN to match Bellman's equation using a randomly sampled batch size ($=256$), and this is done with gradient descent.\n",
    "\n",
    "**Validation step**\n",
    "Access the performance of the learnt policy. Initialize 1024 agents, and let the agent fully decide the actions (we do not replace actions with probability $\\epsilon$ with uniform distribution). Since this is the validation step, this should not interfere with the training process, and the experience buffer/model weights won't be updated.\n",
    "\n",
    "\n",
    "## Notes\n",
    "For an optimal agent, $E[\\text{steps}] = \\frac{1}{2^n}\\sum_{k=0}^n \\dbinom{n}{k}k = \\frac{n}{2}$. We expect the average number of steps to be close to $\\frac{n}{2}$ if an optimal agent is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import src.runtime as runtime\n",
    "from src.runtime import train_DQN_agent\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing $n=5$, UVFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, env = train_DQN_agent(5, device=device, episodes=3000, use_HER=False, model_type=runtime.UVFA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing $n=10$, UVFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, env = train_DQN_agent(10, device=device, episodes=3000, use_HER=False, model_type=runtime.UVFA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing $n=15$, UVFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, env = train_DQN_agent(15, device=device, episodes=3000, use_HER=False, model_type=runtime.UVFA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing $n=20$, UVFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, env = train_DQN_agent(20, device=device, episodes=3000, use_HER=False, model_type=runtime.UVFA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing $n=20$, Handcrafted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, env = train_DQN_agent(20, device=device, episodes=3000, use_HER=False, model_type=runtime.HANDCRAFTED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing $n=30$, Handcrafted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, env = train_DQN_agent(30, device=device, episodes=3000, use_HER=False, model_type=runtime.HANDCRAFTED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
